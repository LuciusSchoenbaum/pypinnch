# File base.py Created by Lucius Schoenbaum April 10, 2023





import torch

from .buffer import Buffer

from copy import deepcopy


from mv1fw import (
    get_fslabels,
    parse_fslabels,
)
from mv1fw.fw import \
    XFormat


class ICBase:
    """
    Base sample set type, it may just house typical data,
    i.e. a few inputs, and one or more outputs, e.g., "x, y, u".
    However, the Solution can also optionally
    house derived values if these values are trained
    on as ICs. Examples::

        "x, y, u, u_x"
        "x, y, u, u_y"
        "x, y, u, u_t, u_x, u_xx"

    These values are then included
    in the solution (the data generated by the solver).
    ...After all, we have to compute them and
    someday we might want to look at these artifacts.

    """

    # todo this class has been part of the code for a long time,
    #  it would benefit from a top-to-bottom review.

    # todo inherit from Sampler?

    # todo inherit from XNoF

    # todo remove Buffer from project

    def __init__(self):
        self.dtype = None
        self.lbl = None
        self.result_measure = None
        self.indim = None
        # Reminder: this is not problem's outdim, but the ic's outdim
        # which are determined by the size of the ic dict defined in the script.
        self.outdim = None
        self.batchsize = None
        # pointer for batching
        self.point = None
        # counter for age (1 age == 1 trip through the dataset)
        self.age = None
        # marker for determining epoch transition point.
        # Invariant: only problem instance can unset this.
        self.epoch_marker = None
        self.device = None

        ##############################
        # fields that are shared with Buffer:

        # timeslice
        self.X = None
        # time of timeslice X
        self.t = 0.0

        ##############################



    def init(
            self,
            problem,
            SPL,
            dtype,
    ):
        """
        (Called by :any:`Driver` )

        This call creates the sample set of the
        solution from the ICs at tinit.
        This method generates the object
        containing input variables and reference output
        variables that are trained as IC constraints.

        .. note::

            This method is called *once* per Driver.

        Arguments:

            dtype:
                Numpy data type
            problem:
                problem instance for
                ic constriaints dict and get() method
            SPL:
                sample density parameter
                (1-dimensional sample per measure)

        """
        self.dtype = dtype
        self.t = problem.th.tinit
        # pointer for batching
        self.point = 0
        # counter for age (1 age == 1 trip through the dataset)
        self.age = 0
        # marker for determining epoch transition point.
        # Invariant: only batch manager instance can unset this.
        self.epoch_marker = False
        if problem.ic_source is None:
            # > 0-d case
            X = None
            n = 1
            self.indim = 0
        else:
            # > create an XFormat - this is necessary to satisfy the API of icref.
            lbl, indim, with_t = parse_fslabels(problem.fslabels)
            X = XFormat(
                X=problem.ic_source(SPL),
                t=self.t,
                fslabels=get_fslabels(lbl[:indim], indim, with_t),
            )
            n = X.size()
            self.indim = indim
        iccs = problem.ic_constraints
        length = len(iccs)
        self.lbl = []
        for ic_constraint_label in iccs:
            self.lbl += ic_constraint_label
        # todo XFormat
        QQref = torch.empty([n, length])
        for i, ic_constraint_label in enumerate(iccs):
            icref = iccs[ic_constraint_label]
            if isinstance(icref, float) or isinstance(icref, int):
                Qref = torch.full((n, 1), float(icref))
            else:
                # > evaluate the IC on the XFormat, ...this likely enters user-defined code
                # todo this returns an XFormat?...
                Qref = icref(X=X, problem=problem)
                # todo for 0-d case, X is None, so problem.get cannot be used in the icref method.
                #  if the user tries to use it, there should be an informative error.
                #  for now, the user just has to remember to return floats in 0-d case (when the domain is a time interval.)
            # todo XFormat
            QQref[:,i:i+1] = Qref
        if X is None:
            # 0-dimensional case
            self.X = QQref
            # self.X.requires_grad_(True)
        else:
            # todo XFormat
            self.X = torch.hstack((X.X(), QQref))
        # todo XFormat !!
        self.outdim = len(iccs)
        if self.X.shape[1] != self.indim + self.outdim:
            raise ValueError(f"Result cannot be initialized with {self.indim} inputs and {self.outdim} outputs.")


    def init_phase(
        self,
        icbase,
        batchsize,
        th,
        device,
    ):
        """
        Initialize a phase-specific base.
        Never called on icbase.

        Remind: The final phase requires fraction = 1.0.

        Arguments:

            icbase (:any:`Base`):
                todo check type
            batchsize (integer):
            th (:any:`TimeHorizon`):
            device:
                device of training
        """
        # todo better object-oriented structure (subclass, base vs icbase (?))

        # todo XNoF

        self.batchsize = batchsize
        self.result_measure = icbase.result_measure
        self.t = th.tinit
        self.device = device
        # pointer for batching
        self.point = 0
        # counter for age (1 age == 1 trip through the dataset)
        self.age = 0
        # marker for determining epoch transition point.
        # Invariant: only batch manager instance can unset this.
        self.epoch_marker = False
        # todo finesse?
        self.indim = icbase.indim
        self.outdim = icbase.outdim
        # self.copies_required = False
        # > populate self.X
        icsize = icbase.X.shape[0]
        if icsize == 1 and self.indim == 0:
            requires_grad_0dim = False
            # zero-dimensional case: the ic is a scalar...
            self.X = torch.full((self.batchsize, self.outdim), float(icbase.X[0,0]), dtype=self.dtype, requires_grad=requires_grad_0dim)
            # ...or a list of scalars.
            if self.outdim > 1:
                for i in range(1, self.outdim):
                    self.X[:,i:i+1] = torch.full((self.batchsize, 1), float(icbase.X[0,i]), dtype=self.dtype, requires_grad=requires_grad_0dim)
            self.X.to(self.device)
        else:
            # make_more_copies = False
            if self.batchsize > icsize:
                # self.copies_required = True
                # make_more_copies = True
                print(f"[Warning] batchsize {self.batchsize} exceeds total size of ic base sample size {icsize}. (If dimension of problem is small, this may be unavoidable.)")
            size = icbase.X.shape[0]
            # if fraction <= 0.0:
            #     raise ValueError(f"The fraction f = {fraction} must satisfy 0 < f <= 1.")
            # if fraction < 1.0:
            #     # todo push a counter between phases for more variability/randomness in the choice of subset of icbase
            #     size = int(float(size)*fraction)
            #     if self.batchsize > size:
            #         print(f"[Warning] batchsize {self.batchsize} exceeds total size of base sample size {size}. (If dimension of problem is small, this may be unavoidable.)")
            #         self.copies_required = True
            #         make_more_copies = True
            self.X = torch.clone(icbase.X[:size,:]).detach() # already detached (was never attached)
            # safety = 6
            # while make_more_copies:
            #     self.X = torch.vstack((self.X, self.X))
            #     if self.X.shape[0] >= self.batchsize:
            #         make_more_copies = False
            #     safety -= 1
            #     if safety == 0:
            #         raise ValueError(f"batchsize {self.batchsize} not compatible with ic base sample size {icsize}.")
            self.X.to(self.device)


    def __call__(self, buffer):
        """
        Calling the base on a buffer

        Arguments:

            buffer (:any:`Buffer`):

        :meta private:
        """
        if not isinstance(buffer, Buffer):
            raise ValueError(f"Expecting Buffer.")
        self.X = torch.clone(buffer.X)
        self.t = deepcopy(buffer.t)


    def deinit(self):
        """
        Called by phase.deinit.
        """
        # free memory
        self.X = None


    def _shuffle(self):
        idxs = torch.randperm(self.X.shape[0])
        self.X = self.X[idxs]
        # self.rng.shuffle(self.X)


    # API for batching


    def batch(self):
        """
        Training on the base is different than
        training on cylindrical samples.
        The return value is of the form (coords, truevalues)
        on the (known, to caller) base geometry.

        Returns:

            XX, QQref:
                (inputs, target) pair for ML training.
                QQref may contain (NN output) values,
                or in general any other variable that the IC's are trained on.

        """
        if self.indim == 0:
            XX = torch.full([self.batchsize, 1], self.t).to(self.device)
            QQref = torch.zeros([self.batchsize,self.X.shape[1]]).to(self.device)
            # > repeat value to fill up a batch: 0d base X is merely a point
            for i in range(self.batchsize):
                QQref[i:i+1,:] = self.X[0:1,:]
            self.age += 1
            self.epoch_marker = True
            # no:
            # XX.requires_grad_(True)
            # QQref.requires_grad_(True)
            return XX, QQref
        else:
            # todo review to preserve X on the device
            # dtypecheck("X", self.X)
            self.X = self.X.to(self.device)
            beg = self.point
            end = self.point + self.batchsize
            XX = torch.full([self.batchsize, 1], self.t).to(self.device)
            XX = torch.hstack((self.X[beg:end,:self.indim], XX)).to(self.device)
            QQref = self.X[beg:end,self.indim:]
            self.point = end
            if self.point + self.batchsize > self.size():
                self._shuffle()
                self.point = 0
                self.age += 1
                self.epoch_marker = True
            return XX, QQref


    def end_of_epoch(self):
        return self.epoch_marker


    def total_ages(self):
        """

        Returns:
            ages trained for current timestep,
            measured by # of batches pulled via batch().
        """
        return self.age


    def reset_ages(self):
        self.age = 0


    def size(self):
        if self.indim == 0:
            return self.batchsize
        else:
            return self.X.shape[0]


    def advance(
            self,
            hub,
            config,
            problem,
    ):
        """
        This pushes a new IC for the next timestep.
        The new IC the old result.

        Arguments:

            hub (:any:`Hub`):
                Hub instance containing
                the neural network for the solution,
                needed to update the result.
            config (:any:`DriverConfig`):
                driver's configuration, including the device
            problem (:any:`Problem`):
                Problem instance

        """
        modules = hub.modules
        device = config.device
        dt = hub.dt
        if len(modules) > 1:
            # todo multiple models is deprecated.
            raise ValueError(f"Base: multiple models not supported yet!")
        else:
            module = modules[0]
        # reset age counters on advance
        # todo review, are all the age counters are reset this way on advance (step)? review
        self.reset_ages()
        self.t += dt
        if problem.indim > 0:
            tvector = torch.full([self.X.shape[0], 1], self.t).to(device)
            XX = self.X[:,:problem.indim]
            # prepare for recursive hstack on self.X
            self.X = XX.clone().detach().to(device).requires_grad_(False)
            XX = torch.hstack((XX, tvector)).to(device).requires_grad_(True)
            # > place on hub
            hub._x = XX
            hub._u = module.forward(hub._x)
            for ic_constraint_label in problem.ic_constraints:
                Q = problem.get(ic_constraint_label, hub).detach().to(device)
                # update result
                self.X = torch.hstack((self.X, Q))
            # check:
            # dtypecheck("base X", self.X)
            problem.clear_gradients()
        else:
            # 0d case - time is only input
            t = torch.full([1,1], self.t).to(device).requires_grad_(True)
            # sic:
            self.X = torch.empty([t.shape[0], 0]).to(device).requires_grad_(False)
            # self.X = torch.empty([t.shape[0],0]).to(device=device)
            hub._x = t
            hub._u = module.forward(hub._x)
            for ic_constraint_label in problem.ic_constraints:
                Q = problem.get(ic_constraint_label, hub).detach().to(device=device)
                # update result
                self.X = torch.hstack((self.X, Q))
            # check:
            # dtypecheck("base X", self.X)
            # todo removing this clear? should be performed by caller?
            problem.clear_gradients()


    # def get(self, variables, _x = None):
    #     """
    #     .. warning::
    #         Deprecated.
    #
    #     For inputs _x, retrieve values
    #     on the base. If any values are not found,
    #     an error is raised.
    #
    #     Example use cases::
    #
    #         get_base("u", -XX) # for first 1d1v nonlocal source term.
    #         get_base("u") # then integrate: for second 1d1v nonlocal source term.
    #
    #     Arguments:
    #
    #         variables:
    #             String of requested variables, e.g., "u, v".
    #             Must be variables maintained by the base.
    #             (Such values are precisely those on which ic's are defined, ITCINOOD.)
    #         _x:
    #             List of inputs ("coordinates") on which to retrieve
    #             values. This will be treated just like keys applied to a dict,
    #             and return the corresponding values using the base.
    #             If None, then the all available values are returned.
    #
    #     Returns:
    #
    #         Tuple of columns containing requested values,
    #         or a single column in case of a singleton tuple.
    #     """
    #     s = variables
    #     out = ()
    #     varlist = [x.strip() for x in s.split(",")]
    #     if len(varlist) == 0:
    #         raise ValueError(f"Empty request to find variable/gradient from input labels {s}.")
    #     for v in varlist:
    #         found = False
    #         for i, lb in enumerate(self.lbl):
    #             if lb == v:
    #                 found = True
    #                 if _x is not None:
    #                     q = 1234 # selection using numpy close().
    #                     for j in range(_x.shape[0]):
    #                         lookup_good = True
    #                         if not lookup_good:
    #                             raise ValueError(f"Value of variable {v} was not found for input values {_x[j,:]}")
    #                 else:
    #                     q = 1234 # all, easy case
    #                 out += (q,)
    #             if not found:
    #                 raise ValueError(f"No value found for requested variable {v}.")
    #     # return a tensor/array if singleton
    #     if len(out) == 1:
    #         out = out[0]
    #     return out




