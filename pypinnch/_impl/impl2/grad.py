




import torch



class Grad:
    """
    Data structure holding auto-cached gradients.
    A new key-value pair will be created for a new pair of (output, input).
    For the (output, input) pair that has been computed before,
    it will reuse the previous instance rather than creating a new one.

    Frameworks for AD are normally tensor-tensor:
    a derivative (gradient) is a derivative of a tensor ``y`` with respect
    to a tensor ``x``. The canonical application (in ML) is to compute the
    derivative of the loss ``L`` with respect to the model's weights ``W``,
    and this computation is typically wrapped up somewhere within
    the framework as part of the optimization support.

    However, if ML is applied to a PINN, inputs and outputs are
    significant component-wise whenever the AD is used to compute
    the derivative of an input with respect to an output, in order to
    generate a physics-driven constraint on the model, i.e., a pde constraint.
    Moreover, there are cases of PINNs where the
    same values will be needed at different stages of the training loop.
    For instance, if there is a coupled set of pdes, it would be
    natural that a certain partial derivative, based on the same batch of
    inputs, and with the same variable input and variable output,
    might be needed at two or more points in the call stack
    generated by a computation of the iteration's loss.

    In order to avoid recomputing the entire tensor-tensor derivative
    each time a different slice (or perhaps even the same slice as before)
    is needed, the solution is to cache (store) the tensor value
    in program memory, then to take the part that is needed at
    the time of the call. For example, if you use the computation
    of the gradient of u with respect to x,y,z and then extract u_x,
    you can later extract u_y or u_z, without repeating the calculation,
    because the tensor derivative u_{x,y,z} was saved after the first
    time of call.

    This approach is hereby credited originally to the project
    `DeepXDE <https://deepxde.readthedocs.io>`_.

    .. note::
        Saved derivatives cannot be reused across iteration,
        i.e., when the data is revised. So you must reset by calling
        ``clear()`` in each iteration to avoid a memory leak.

    """

    # todo more unit testing

    # automatic differentiation (AD)
    # depends on applying differentiable operations to tensors that can be added
    # to a computational graph. An operation that does not seem to satisfy
    # this requirement is slicing (for example, given an array x,y,z,
    # slicing to obtain the values x, or the values y, or the values z.),
    # yet PyTorch seems to allow arbitrary slicing AFAIK,
    # which seems remarkable to me.
    # I would like to understand better how this is possible.

    def __init__(self):
        # Retain memory of what has already been computed.
        self.grads = {}

    def __call__(self, _x, _y, higher=False):
        key = (_x, _y)
        # print("[grad] received key _x, _y")
        # if True:
        if key not in self.grads or (higher and not self.grads[key].requires_grad):
            # if key not in self.grads:
            #     print("[grad] LOGIC key not in self.grads")
            # if higher and not self.grads[key].requires_grad:
            #     print("[grad] LOGIC higher and not self.grads[key].requires_grad")
            # print("[grad] calling torch.autograd.grad")
            # value is not found cached,
            # or the higher request is stronger than that of the cached value.
            # cache the value
            y_x = torch.autograd.grad(
                outputs=_y,
                inputs=_x,
                grad_outputs=torch.ones_like(_y),
                create_graph=True, # higher, todo just for today, overriding this arg during investigations
                retain_graph=True, # todo investigate more create_graph vs. retain_graph vs. requires_grad()
                # Notes 5/27/23:
                # My original setting was:
                # create_graph=higher,
                # retain_graph=True,
                # I found that if create_graph is False, however,
                # the residuals and losses obtained are not differentiable.
                # Since this is unacceptable, we must have create_graph=True.
                # This leaves retain_graph.
                # I am not sure if retain_graph can be set equal to higher
                # with some benefit, or not.
                # Right now, I am mainly concerned with getting early-stage runs working,
                # so I will return to this issue in the future.
                # I set retain_graph to True in order to hide this issue from my current purview.
                # ...I am pretty sure it is True, however, if create_graph is True, but I don't recall right now.
                # This rabbit hole goes fairly deep.
                #
                # todo May 26, 2024, test: set this to True
                # test result: _______no effect________
                allow_unused=True,
            )[0]
            self.grads[key] = y_x
            return y_x
        else:
            # print("[grad] LOGIC key found and (higher and not self.grad[key].requires_grad)")
            return self.grads[key]

    def clear(self):
        self.grads = {}




